{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95527838-4ecc-4615-a34d-ed3970ae3ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# stats import\n",
    "from scipy.stats import permutation_test\n",
    "def statistic(condA, condB, axis):\n",
    "    return np.nanmean(condA, axis=axis) - np.nanmean(condB, axis=axis)\n",
    "\n",
    "# Define parameters\n",
    "subjects = ['sub-001', 'sub-002', 'sub-003', 'sub-004',\n",
    "            'sub-005', 'sub-006', 'sub-007', 'sub-008']\n",
    "subjects_plot = ['sub-001', 'sub-002', 'sub-003', 'sub-004',\n",
    "                 'sub-005', 'sub-006', 'sub-007', 'sub-008', 'group']\n",
    "tasks = ['FullScreen', 'FullScreenAttendFix', 'FullScreenAttendBar']\n",
    "rois = ['V1', 'V2', 'V3', 'V3AB', 'hMT+', 'LO',\n",
    "        'VO', 'iIPS', 'sIPS', 'iPCS', 'sPCS', 'mPCS']\n",
    "\n",
    "# Define folders\n",
    "base_dir = '/home/mszinte/disks/meso_S/data/gaze_prf'\n",
    "bids_dir = \"{}\".format(base_dir)\n",
    "pp_dir = \"{}/derivatives/pp_data\".format(base_dir)\n",
    "\n",
    "# analysis settings\n",
    "best_voxels_num = 250\n",
    "type_analyses = ['','_best{}'.format(best_voxels_num)]\n",
    "n_permutation = 10000\n",
    "cortical_mask = 'cortical'\n",
    "n_ecc_bins=10\n",
    "verbose = False\n",
    "TR = 1.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4781fbae-58f3-4a6d-abe1-78c3837f2d45",
   "metadata": {},
   "source": [
    "### Compute TSV files for fullscreen atention R2 comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb4c46-0199-42cb-9b4d-69f2aae725a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create TSV files\n",
    "group_tsv_dir = '{}/{}/prf/tsv'.format(pp_dir, 'group')\n",
    "try: os.makedirs(group_tsv_dir)\n",
    "except: pass\n",
    "\n",
    "for task in tasks:\n",
    "    for subject_num, subject in enumerate(subjects):\n",
    "        # define folders\n",
    "        fit_dir = '{}/{}/prf/fit'.format(pp_dir, subject)\n",
    "        mask_dir = '{}/{}/masks'.format(pp_dir, subject)\n",
    "        tsv_dir = '{}/{}/prf/tsv'.format(pp_dir, subject)\n",
    "        try: os.makedirs(tsv_dir)\n",
    "        except: pass\n",
    "\n",
    "        # load pRF threshold masks\n",
    "        th_mat = nb.load('{}/{}_task-{}_prf_threshold.nii.gz'.format(mask_dir,subject,task)).get_fdata()\n",
    "\n",
    "        # load fit parameters x by threshold\n",
    "        r2_th_mat = nb.load('{}/{}_task-{}_par-r2.nii.gz'.format(fit_dir,subject,task)).get_fdata()*th_mat\n",
    "        ecc_th_mat = nb.load('{}/{}_task-{}_par-ecc.nii.gz'.format(fit_dir,subject,task)).get_fdata()*th_mat\n",
    "        sd_th_mat = nb.load('{}/{}_task-{}_par-sd.nii.gz'.format(fit_dir,subject,task)).get_fdata()*th_mat\n",
    "        x_th_mat = nb.load('{}/{}_task-{}_par-x.nii.gz'.format(fit_dir,subject,task)).get_fdata()*th_mat\n",
    "        y_th_mat = nb.load('{}/{}_task-{}_par-y.nii.gz'.format(fit_dir,subject,task)).get_fdata()*th_mat\n",
    "        amp_th_mat = nb.load('{}/{}_task-{}_par-amplitude.nii.gz'.format(fit_dir,subject,task)).get_fdata()*th_mat\n",
    "        bsl_th_mat = nb.load('{}/{}_task-{}_par-baseline.nii.gz'.format(fit_dir,subject,task)).get_fdata()*th_mat\n",
    "        \n",
    "        # creat tsv\n",
    "        for roi_num, roi in enumerate(rois):\n",
    "            # load roi\n",
    "            lh_mat = nb.load(\"{}/{}_{}_L.nii.gz\".format(mask_dir, roi, cortical_mask)).get_fdata()\n",
    "            rh_mat = nb.load(\"{}/{}_{}_R.nii.gz\".format(mask_dir, roi, cortical_mask)).get_fdata()\n",
    "            roi_mat = lh_mat + rh_mat\n",
    "            roi_mat[roi_mat==0] = np.nan\n",
    "\n",
    "            # select data by roi mask\n",
    "            r2_roi_th_mat = r2_th_mat[roi_mat==True]\n",
    "            ecc_roi_th_mat = ecc_th_mat[roi_mat==True]\n",
    "            sd_roi_th_mat = sd_th_mat[roi_mat==True]\n",
    "            x_roi_th_mat = x_th_mat[roi_mat==True]\n",
    "            y_roi_th_mat = y_th_mat[roi_mat==True]\n",
    "            amp_roi_th_mat = amp_th_mat[roi_mat==True]\n",
    "            bsl_roi_th_mat = bsl_th_mat[roi_mat==True]\n",
    "            \n",
    "            # create dataframe\n",
    "            df_roi = pd.DataFrame({'subject': [subject] * r2_roi_th_mat.shape[0],\n",
    "                                   'roi': [roi] * r2_roi_th_mat.shape[0],\n",
    "                                   'r2': r2_roi_th_mat,\n",
    "                                   'ecc': ecc_roi_th_mat,\n",
    "                                   'sd': sd_roi_th_mat,\n",
    "                                   'x': x_roi_th_mat,\n",
    "                                   'y': y_roi_th_mat,\n",
    "                                   'amplitude': amp_roi_th_mat,\n",
    "                                   'baseline': bsl_roi_th_mat})\n",
    "\n",
    "            # rank based on r2\n",
    "            if task == 'FullScreen': \n",
    "                df_roi['rank_fs_r2']=df_roi.groupby('roi')['r2'].rank(method='max',ascending=False)\n",
    "            else:\n",
    "                df_fs = pd.read_csv(\"{}/{}_task-FullScreen_prf_threshold_par.tsv\".format(tsv_dir,subject,task),sep=\"\\t\")\n",
    "                df_roi['rank_fs_r2'] = np.array(df_fs.loc[(df_fs.roi == roi)]['rank_fs_r2'])\n",
    "                \n",
    "            # get best voxels\n",
    "            df_best_roi = df_roi[(df_roi.rank_fs_r2<=best_voxels_num)]\n",
    "            \n",
    "            # across roi\n",
    "            if roi_num > 0: \n",
    "                df = pd.concat([df,df_roi], ignore_index=True)\n",
    "                df_best = pd.concat([df_best,df_best_roi], ignore_index=True)                \n",
    "            else:\n",
    "                df = df_roi\n",
    "                df_best = df_best_roi\n",
    "        \n",
    "        # save dataframe\n",
    "        df_fn = \"{}/{}_task-{}_prf_threshold_par.tsv\".format(tsv_dir,subject,task)\n",
    "        print('saving {}'.format(df_fn))\n",
    "        df.to_csv(df_fn, sep=\"\\t\", na_rep='NaN',index=False)\n",
    "        \n",
    "        df_best_fn = \"{}/{}_task-{}_prf_threshold_par_best{}.tsv\".format(tsv_dir,subject,task,int(best_voxels_num))\n",
    "        print('saving {}'.format(df_best_fn))\n",
    "        df_best.to_csv(df_best_fn, sep=\"\\t\", na_rep='NaN',index=False)\n",
    "        \n",
    "        # across subject\n",
    "        if subject_num == 0: df_group = df\n",
    "        else: df_group = pd.concat([df_group, df])\n",
    "        \n",
    "        if subject_num == 0: df_best_group = df_best\n",
    "        else: df_best_group = pd.concat([df_best_group, df_best])\n",
    "        \n",
    "    # save group data\n",
    "    df_group_fn = \"{}/group_task-{}_prf_threshold_par.tsv\".format(group_tsv_dir,task)\n",
    "    print('saving {}'.format(df_group_fn))\n",
    "    df_group.to_csv(df_group_fn, sep=\"\\t\", na_rep='NaN')\n",
    "    \n",
    "    df_best_group_fn = \"{}/group_task-{}_prf_threshold_par_best{}.tsv\".format(group_tsv_dir,task,int(best_voxels_num))\n",
    "    print('saving {}'.format(df_best_group_fn))\n",
    "    df_best_group.to_csv(df_best_group_fn, sep=\"\\t\", na_rep='NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1fa41-2323-4259-be9f-4087dfcfb306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute permutation statistics results\n",
    "for type_analysis in type_analyses:\n",
    "    # load data across participants\n",
    "    ab_df = pd.read_csv(\"{}/group_task-FullScreenAttendBar_prf_threshold_par{}.tsv\".format(group_tsv_dir,type_analysis),sep=\"\\t\")\n",
    "    af_df = pd.read_csv(\"{}/group_task-FullScreenAttendFix_prf_threshold_par{}.tsv\".format(group_tsv_dir,type_analysis),sep=\"\\t\")\n",
    "    r2_ab_df = ab_df.groupby(['roi','subject']).r2.mean().reset_index(name='r2')\n",
    "    r2_af_df = af_df.groupby(['roi','subject']).r2.mean().reset_index(name='r2')\n",
    "\n",
    "    for roi_num, roi in enumerate(rois):\n",
    "\n",
    "        ab_r2_array = np.array(r2_ab_df[r2_af_df.roi==roi].r2)\n",
    "        af_r2_array = np.array(r2_af_df[r2_af_df.roi==roi].r2)\n",
    "        perm_res = permutation_test((ab_r2_array, af_r2_array), statistic, n_resamples=n_permutation, alternative='two-sided',\n",
    "                                    permutation_type='samples', vectorized=True, axis=0)\n",
    "\n",
    "        p_val_unilateral = perm_res.pvalue/2\n",
    "        p_val_bilateral = perm_res.pvalue\n",
    "\n",
    "        if p_val_unilateral > 0.05: p_text_unilateral = 'p = {:1.2f}'.format(p_val_unilateral)\n",
    "        if p_val_unilateral < 0.05: p_text_unilateral = 'p < 0.05'\n",
    "        if p_val_unilateral < 0.01: p_text_unilateral = 'p < 0.01'\n",
    "        if p_val_unilateral < 0.001: p_text_unilateral = 'p < 0.001'\n",
    "        if p_val_unilateral < 0.0001: p_text_unilateral = 'p < 0.0001'\n",
    "\n",
    "        if p_val_bilateral > 0.05: p_text_bilateral = 'p = {:1.2f}'.format(p_val_bilateral)\n",
    "        if p_val_bilateral < 0.05: p_text_bilateral = 'p < 0.05'\n",
    "        if p_val_bilateral < 0.01: p_text_bilateral = 'p < 0.01'\n",
    "        if p_val_bilateral < 0.001: p_text_bilateral = 'p < 0.001'\n",
    "        if p_val_bilateral < 0.0001: p_text_bilateral = 'p < 0.0001'\n",
    "\n",
    "        df_stats = pd.DataFrame({  'roi': [roi],\n",
    "                                   'ab_r2_mean': np.nanmean(ab_r2_array),\n",
    "                                   'ab_r2_sem': np.nanstd(ab_r2_array)/np.sqrt(ab_r2_array.shape[0]-1),\n",
    "                                   'af_r2_mean': np.nanmean(af_r2_array),\n",
    "                                   'af_r2_sem': np.nanstd(af_r2_array)/np.sqrt(af_r2_array.shape[0]-1),\n",
    "                                   'ab_af_diff': perm_res.statistic,\n",
    "                                   'p_val_unilateral': p_val_unilateral,\n",
    "                                   'p_text_unilateral': p_text_unilateral,\n",
    "                                   'p_val_bilateral': p_val_bilateral,\n",
    "                                   'p_text_bilateral': p_text_bilateral\n",
    "                                  })\n",
    "\n",
    "        # across subject\n",
    "        if roi_num == 0: df_stats_roi = df_stats\n",
    "        else: df_stats_roi = pd.concat([df_stats_roi, df_stats])\n",
    "    df_stats_roi_fn = \"{}/group_task-FullScreen_prf_threshold_stats{}.tsv\".format(group_tsv_dir,type_analysis)\n",
    "    print('saving {}'.format(df_stats_roi_fn))\n",
    "    df_stats_roi.to_csv(df_stats_roi_fn, sep=\"\\t\", na_rep='NaN')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f41070-a6c0-47c1-9a14-c36ca20e4727",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Eccentricity effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ae3f68-c79b-42f4-ac9d-6d4646bdb8b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute permutation tests\n",
    "tsv_dir = '{}/group/prf/tsv'.format(pp_dir, subject)\n",
    "for type_analysis in type_analyses:\n",
    "    af_df = pd.read_csv(\"{}/group_task-FullScreenAttendFix_prf_threshold_par{}.tsv\".format(tsv_dir,type_analysis), sep=\"\\t\")\n",
    "    ab_df = pd.read_csv(\"{}/group_task-FullScreenAttendBar_prf_threshold_par{}.tsv\".format(tsv_dir,type_analysis), sep=\"\\t\")\n",
    "    af_df.roi = pd.Categorical(af_df.roi,categories=rois)\n",
    "    ab_df.roi = pd.Categorical(af_df.roi,categories=rois)\n",
    "\n",
    "    for roi_num, roi in enumerate(rois):\n",
    "        ab_df_binned = ab_df[ab_df.roi==roi].assign(ecc_bin=pd.qcut(ab_df.ecc, n_ecc_bins))\n",
    "        af_df_binned = af_df[af_df.roi==roi].assign(ecc_bin=pd.qcut(af_df.ecc, n_ecc_bins))\n",
    "\n",
    "        ab_df_binned_mean = ab_df_binned.groupby(['subject','ecc_bin']).r2.mean().reset_index(name='r2_mean').assign(ecc_bin_num=np.concatenate([np.arange(0,n_ecc_bins)]*8))\n",
    "        af_df_binned_mean = af_df_binned.groupby(['subject','ecc_bin']).r2.mean().reset_index(name='r2_mean').assign(ecc_bin_num=np.concatenate([np.arange(0,n_ecc_bins)]*8))\n",
    "\n",
    "        for ecc_bin_num in np.arange(0,n_ecc_bins):\n",
    "            ab_df_binned_r2_array = np.array(ab_df_binned_mean[ab_df_binned_mean.ecc_bin_num==ecc_bin_num].r2_mean)\n",
    "            af_df_binned_r2_array = np.array(af_df_binned_mean[af_df_binned_mean.ecc_bin_num==ecc_bin_num].r2_mean)\n",
    "\n",
    "            perm_res = permutation_test((ab_df_binned_r2_array, af_df_binned_r2_array), statistic, n_resamples=n_permutation, alternative='two-sided',\n",
    "                                         permutation_type='samples', vectorized=True, axis=0)\n",
    "\n",
    "            p_val_unilateral = perm_res.pvalue/2\n",
    "            p_val_bilateral = perm_res.pvalue\n",
    "\n",
    "            if p_val_unilateral > 0.05: p_text_unilateral = 'p = {:1.2f}'.format(p_val_unilateral)\n",
    "            if p_val_unilateral < 0.05: p_text_unilateral = 'p < 0.05'\n",
    "            if p_val_unilateral < 0.01: p_text_unilateral = 'p < 0.01'\n",
    "            if p_val_unilateral < 0.001: p_text_unilateral = 'p < 0.001'\n",
    "            if p_val_unilateral < 0.0001: p_text_unilateral = 'p < 0.0001'\n",
    "\n",
    "            if p_val_bilateral > 0.05: p_text_bilateral = 'p = {:1.2f}'.format(p_val_bilateral)\n",
    "            if p_val_bilateral < 0.05: p_text_bilateral = 'p < 0.05'\n",
    "            if p_val_bilateral < 0.01: p_text_bilateral = 'p < 0.01'\n",
    "            if p_val_bilateral < 0.001: p_text_bilateral = 'p < 0.001'\n",
    "            if p_val_bilateral < 0.0001: p_text_bilateral = 'p < 0.0001'\n",
    "\n",
    "            df_stats = pd.DataFrame({  'roi': [roi],\n",
    "                                       'ecc_bin_num': [ecc_bin_num],\n",
    "                                       'ab_r2_mean': np.nanmean(ab_df_binned_r2_array),\n",
    "                                       'ab_r2_sem': np.nanstd(ab_df_binned_r2_array)/np.sqrt(ab_df_binned_r2_array.shape[0]-1),\n",
    "                                       'af_r2_mean': np.nanmean(af_df_binned_r2_array),\n",
    "                                       'af_r2_sem': np.nanstd(af_df_binned_r2_array)/np.sqrt(af_df_binned_r2_array.shape[0]-1),\n",
    "                                       'ab_af_diff': perm_res.statistic,\n",
    "                                       'p_val_unilateral': p_val_unilateral,\n",
    "                                       'p_text_unilateral': p_text_unilateral,\n",
    "                                       'p_val_bilateral': p_val_bilateral,\n",
    "                                       'p_text_bilateral': p_text_bilateral,\n",
    "                                      })\n",
    "            # across eccentricity\n",
    "            if ecc_bin_num == 0: df_stats_ecc_bin_num = df_stats\n",
    "            else: df_stats_ecc_bin_num = pd.concat([df_stats_ecc_bin_num, df_stats])\n",
    "\n",
    "        # across roi\n",
    "        if roi_num == 0: df_stats_ecc_bin_num_roi = df_stats_ecc_bin_num\n",
    "        else: df_stats_ecc_bin_num_roi = pd.concat([df_stats_ecc_bin_num_roi, df_stats_ecc_bin_num])    \n",
    "    \n",
    "    df_stats_ecc_bin_num_roi_fn = \"{}/group_task-FullScreen_prf_threshold_ecc_bin_num_stats{}.tsv\".format(group_tsv_dir,type_analysis)\n",
    "    print('saving {}'.format(df_stats_ecc_bin_num_roi_fn))\n",
    "    df_stats_ecc_bin_num_roi.to_csv(df_stats_ecc_bin_num_roi_fn, sep=\"\\t\", na_rep='NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709a6aba-e67d-489a-a827-1212a89c5fc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Compute time series / parameters / predictions TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5fd771-c999-45d7-a7ef-f002b8470de0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    \n",
    "    print('\\n{}...'.format(subject))\n",
    "    \n",
    "    # define folders\n",
    "    fit_dir = '{}/{}/prf/fit'.format(pp_dir, subject)\n",
    "    func_avg_dir = '{}/{}/func_avg'.format(pp_dir, subject)\n",
    "    pred_dir = '{}/{}/prf/predictions'.format(pp_dir, subject)\n",
    "    mask_dir = '{}/{}/masks'.format(pp_dir, subject)\n",
    "    tsv_dir = '{}/{}/prf/tsv'.format(pp_dir, subject)\n",
    "\n",
    "    try: os.makedirs(tsv_dir)\n",
    "    except: pass\n",
    "\n",
    "    # load pRF threshold masks\n",
    "    th_mat = nb.load('{}/{}_task-FullScreen_prf_threshold.nii.gz'.format(mask_dir,subject)).get_fdata()\n",
    "\n",
    "    # define empty coordinates index\n",
    "    x_idx, y_idx, z_idx = np.zeros_like(th_mat), np.zeros_like(th_mat), np.zeros_like(th_mat)\n",
    "    for x in np.arange(th_mat.shape[0]):\n",
    "        for y in np.arange(th_mat.shape[1]):\n",
    "            for z in np.arange(th_mat.shape[2]):\n",
    "                x_idx[x,y,z], y_idx[x,y,z], z_idx[x,y,z] = x, y, z\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------\n",
    "    # Load fit parameters\n",
    "    print('\\nLoad fit parameters...')\n",
    "    for attend_task, attend_task_short in zip(['','AttendFix','AttendBar'], ['', '_af', '_ab']):\n",
    "        for fit_param in ['amplitude','baseline','ecc','r2','sd','x','y','theta']:\n",
    "            exec(\"fs{}_{} = nb.load('{}/{}_task-FullScreen{}_par-{}.nii.gz').get_fdata()*th_mat\".format(\n",
    "                attend_task_short,fit_param,fit_dir,subject,attend_task,fit_param))\n",
    "            if verbose: print(\"load fs{}_{}\".format(attend_task_short,fit_param))\n",
    "            \n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------\n",
    "    # Create dataframe with fit parameters\n",
    "    print('\\nCreate dataframe with fit parameters...')\n",
    "    for roi_num, roi in enumerate(rois):\n",
    "        # load roi\n",
    "        lh_mat = nb.load(\"{}/{}_{}_L.nii.gz\".format(mask_dir, roi, cortical_mask)).get_fdata()\n",
    "        rh_mat = nb.load(\"{}/{}_{}_R.nii.gz\".format(mask_dir, roi, cortical_mask)).get_fdata()\n",
    "        roi_mat = lh_mat + rh_mat\n",
    "        roi_mat[roi_mat==0] = np.nan\n",
    "\n",
    "        # create dataframe\n",
    "        df_roi = pd.DataFrame({'subject': [subject] * fs_r2[roi_mat==True].shape[0]})\n",
    "        df_roi['roi'] = [roi] * fs_r2[roi_mat==True].shape[0]\n",
    "        df_roi['vox_coord'] = np.array([x_idx[roi_mat==True],y_idx[roi_mat==True],z_idx[roi_mat==True]]).T.tolist()        \n",
    "\n",
    "        # fullscreen parameters\n",
    "        for attend_task in ['', '_ab', '_af']:\n",
    "            for fit_param in ['amplitude','baseline','ecc','r2','sd','x','y','theta']:\n",
    "                exec(\"df_roi['{}_fs{}'] = fs{}_{}[roi_mat==True]\".format(fit_param, attend_task, attend_task, fit_param))\n",
    "                if roi_num == 11:\n",
    "                    if verbose: print(\"create df['{}_fs{}']\".format(fit_param, attend_task))\n",
    "                    exec(\"del fs{}_{}\".format(attend_task, fit_param))\n",
    "                    if verbose: print(\"delete fs{}_{} from working memory\".format(attend_task, fit_param))\n",
    "        \n",
    "\n",
    "        # get rank with fs\n",
    "        df_roi['rank_r2_fs'] = df_roi.groupby('roi')['r2_fs'].rank(method='max', ascending=False)\n",
    "\n",
    "        # keep best 250\n",
    "        df_roi = df_roi[(df_roi.rank_r2_fs<=best_voxels_num)]\n",
    "\n",
    "        # across roi\n",
    "        if roi_num > 0: df = pd.concat([df,df_roi], ignore_index=True)\n",
    "        else: df = df_roi\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------\n",
    "    # Load time series\n",
    "    print('\\nLoad time series...')\n",
    "    for main_task, main_task_short in zip(['FullScreen', 'GazeCenter', 'GazeLeft', 'GazeRight'], ['fs', 'gc', 'gl', 'gr']):\n",
    "        if main_task == 'FullScreen':\n",
    "            attend_tasks = ['','AttendFix','AttendBar']\n",
    "            attend_tasks_short = ['','_af','_ab']\n",
    "        else:\n",
    "            attend_tasks = ['AttendFix','AttendBar']\n",
    "            attend_tasks_short = ['_af','_ab']\n",
    "\n",
    "        for attend_task, attend_task_short in zip(attend_tasks, attend_tasks_short):\n",
    "            exec(\"{}{}_ts = nb.load('{}/{}_task-{}{}_fmriprep_dct_avg.nii.gz').get_fdata()\".format(\n",
    "                main_task_short, attend_task_short, func_avg_dir, subject, main_task, attend_task))\n",
    "            if verbose: print(\"load {}{}_ts\".format(main_task_short, attend_task_short))\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------\n",
    "    # Add timeseries to the dataframe\n",
    "    print('\\nAdd timeseries to the dataframe...')\n",
    "    fs_r2 = nb.load('{}/{}_task-FullScreen_par-r2.nii.gz'.format(fit_dir,subject)).get_fdata()*th_mat\n",
    "    for roi_num, roi in enumerate(rois):\n",
    "        # load roi\n",
    "        lh_mat = nb.load(\"{}/{}_{}_L.nii.gz\".format(mask_dir, roi, cortical_mask)).get_fdata()\n",
    "        rh_mat = nb.load(\"{}/{}_{}_R.nii.gz\".format(mask_dir, roi, cortical_mask)).get_fdata()\n",
    "        roi_mat = lh_mat + rh_mat\n",
    "        roi_mat[roi_mat==0] = np.nan\n",
    "\n",
    "        # create dataframe\n",
    "        df_roi = pd.DataFrame({'subject': [subject] * fs_r2[roi_mat==True].shape[0]})\n",
    "        df_roi['roi'] = [roi] * fs_r2[roi_mat==True].shape[0]\n",
    "\n",
    "        # fullscreen r2 parameter\n",
    "        df_roi['r2_fs'] = fs_r2[roi_mat==True]\n",
    "\n",
    "        # get time\n",
    "        df_roi['time_fs'] = [np.arange(1,fs_ts.shape[3]+1)*TR]*np.sum(roi_mat==True)\n",
    "        df_roi['time_gaze'] = [np.arange(1,gc_ab_ts.shape[3]+1)*TR]*np.sum(roi_mat==True)\n",
    "\n",
    "        # get data timeseries\n",
    "        for main_task in ['fs', 'gc', 'gl', 'gr']:\n",
    "            if main_task == 'fs':\n",
    "                attend_tasks = ['', '_ab', '_af']\n",
    "            else:\n",
    "                attend_tasks = ['_ab', '_af']\n",
    "\n",
    "            for attend_task in attend_tasks:\n",
    "                exec(\"df_roi['data_{}{}'] = ({}{}_ts[roi_mat==True,:]).tolist()\".format(main_task, attend_task, main_task, attend_task))\n",
    "\n",
    "                if roi_num == 11:\n",
    "                    if verbose: print(\"create df['data_{}{}']\".format(main_task, attend_task))\n",
    "                    exec(\"del {}{}_ts\".format(main_task, attend_task))\n",
    "                    if verbose: print(\"delete {}{}_ts from working memory\".format(main_task, attend_task))\n",
    "\n",
    "        df_roi['rank_r2_fs'] = df_roi.groupby('roi')['r2_fs'].rank(method='max', ascending=False)\n",
    "\n",
    "        # keep best 250\n",
    "        df_roi = df_roi[(df_roi.rank_r2_fs<=best_voxels_num)]\n",
    "\n",
    "        # across roi\n",
    "        if roi_num > 0: df2 = pd.concat([df2,df_roi], ignore_index=True)\n",
    "        else: df2 = df_roi\n",
    "\n",
    "    # add to df\n",
    "    df2.drop(['subject', 'roi', 'r2_fs', 'rank_r2_fs'], axis=1, inplace=True)\n",
    "    df = pd.concat([df,df2], axis=1)\n",
    "    del df2\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------\n",
    "    # Load predictions and r2\n",
    "    print('\\nLoad predictions and r2...')\n",
    "    for main_task, main_task_short in zip(['FullScreen', 'GazeCenter', 'GazeLeft', 'GazeRight'],  ['fs', 'gc', 'gl', 'gr']):\n",
    "        if main_task == 'FullScreen': \n",
    "            attend_tasks = ['','AttendFix','AttendBar']\n",
    "            attend_tasks_short = ['','_af','_ab']\n",
    "            pred_names = ['fit_predictions']\n",
    "            pred_names_short = ['_pred']\n",
    "        else:\n",
    "            attend_tasks = ['AttendFix','AttendBar']\n",
    "            attend_tasks_short = ['_af','_ab']\n",
    "            pred_names = ['retinotopic_FullScreen-predictions',\n",
    "                          'spatiotopic_FullScreen-predictions']\n",
    "            pred_names_short = ['_retino_pred','_spatio_pred']\n",
    "\n",
    "        for attend_task, attend_task_short in zip(attend_tasks, attend_tasks_short):\n",
    "\n",
    "            for pred_name, pred_name_short in zip(pred_names, pred_names_short):\n",
    "                exec(\"{}{}{}_ts = nb.load('{}/{}_task-{}{}_{}.nii.gz').get_fdata()\".format(\n",
    "                        main_task_short, attend_task_short, pred_name_short,\n",
    "                        pred_dir, subject, main_task, attend_task, pred_name))\n",
    "                if verbose: print(\"load {}{}{}_ts\".format(main_task_short, attend_task_short, pred_name_short))\n",
    "                if main_task != 'FullScreen': \n",
    "                    exec(\"{}{}{}_r2 = nb.load('{}/{}_task-{}{}_{}_r2.nii.gz').get_fdata()*th_mat\".format(\n",
    "                            main_task_short, attend_task_short, pred_name_short,\n",
    "                            pred_dir, subject, main_task, attend_task, pred_name))\n",
    "                    if verbose: print(\"load {}{}{}_r2\".format(main_task_short, attend_task_short, pred_name_short))\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------\n",
    "    # Add predictions and r2 to dataframe\n",
    "    print('\\nAdd predictions and r2 to dataframe...')\n",
    "    fs_r2 = nb.load('{}/{}_task-FullScreen_par-r2.nii.gz'.format(fit_dir,subject)).get_fdata()*th_mat\n",
    "    for roi_num, roi in enumerate(rois):\n",
    "        # load roi\n",
    "        lh_mat = nb.load(\"{}/{}_{}_L.nii.gz\".format(mask_dir, roi, cortical_mask)).get_fdata()\n",
    "        rh_mat = nb.load(\"{}/{}_{}_R.nii.gz\".format(mask_dir, roi, cortical_mask)).get_fdata()\n",
    "        roi_mat = lh_mat + rh_mat\n",
    "        roi_mat[roi_mat==0] = np.nan\n",
    "\n",
    "        # create dataframe\n",
    "        df_roi = pd.DataFrame({'subject': [subject] * fs_r2[roi_mat==True].shape[0]})\n",
    "        df_roi['roi'] = [roi] * fs_r2[roi_mat==True].shape[0]\n",
    "\n",
    "        # fullscreen r2 parameter\n",
    "        df_roi['r2_fs'] = fs_r2[roi_mat==True]\n",
    "\n",
    "        # get data timeseries\n",
    "        for main_task in ['fs', 'gc', 'gl', 'gr']:\n",
    "            if main_task == 'fs':\n",
    "                attend_tasks = ['', '_ab', '_af']\n",
    "                pred_names_short = ['pred']\n",
    "            else:\n",
    "                attend_tasks = ['_ab', '_af']\n",
    "                pred_names_short = ['retino_pred','spatio_pred']\n",
    "\n",
    "            for attend_task in attend_tasks:\n",
    "\n",
    "                for pred_name_short in pred_names_short:\n",
    "                    exec(\"df_roi['{}_{}{}'] = ({}{}_{}_ts[roi_mat==True,:]).tolist()\".format(pred_name_short, main_task, attend_task, \n",
    "                                                                                          main_task, attend_task, pred_name_short))\n",
    "                    if roi_num == 11:\n",
    "                        if verbose: print(\"create df['{}_{}{}']\".format(pred_name_short, main_task, attend_task))\n",
    "                        exec(\"del {}{}_{}_ts\".format(main_task, attend_task, pred_name_short))\n",
    "                        if verbose: print(\"delete {}{}_{}_ts from working memory\".format(main_task, attend_task, pred_name_short))\n",
    "                    \n",
    "                    if main_task != 'fs': \n",
    "                        exec(\"df_roi['{}_{}{}_r2'] = ({}{}_{}_r2[roi_mat==True]).tolist()\".format(pred_name_short, main_task, attend_task, \n",
    "                                                                                              main_task, attend_task, pred_name_short))\n",
    "                        if roi_num == 11:\n",
    "                            if verbose: print(\"create df['{}_{}{}_r2']\".format(pred_name_short, main_task, attend_task))\n",
    "                            exec(\"del {}{}_{}_r2\".format(main_task, attend_task, pred_name_short))\n",
    "                            if verbose: print(\"delete {}{}_{}_r2 from working memory\".format(main_task, attend_task, pred_name_short))\n",
    "                        \n",
    "                        \n",
    "        df_roi['rank_r2_fs'] = df_roi.groupby('roi')['r2_fs'].rank(method='max', ascending=False)\n",
    "\n",
    "        # keep best 250\n",
    "        df_roi = df_roi[(df_roi.rank_r2_fs<=best_voxels_num)]\n",
    "\n",
    "        # across roi\n",
    "        if roi_num > 0: df2 = pd.concat([df2,df_roi], ignore_index=True)\n",
    "        else: df2 = df_roi\n",
    "\n",
    "    # add to df\n",
    "    df2.drop(['subject', 'roi', 'r2_fs', 'rank_r2_fs'], axis=1, inplace=True)\n",
    "    df = pd.concat([df,df2], axis=1)\n",
    "    del df2\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------\n",
    "    # Save dataframe as tsv\n",
    "    df_fn = \"{}/{}_all_res_best{}.pkl\".format(tsv_dir,subject,int(best_voxels_num))    \n",
    "    print('saving {}'.format(df_fn))\n",
    "    # df.to_csv(df_fn, sep=\"\\t\", na_rep='NaN',index=False)\n",
    "    df.to_pickle(df_fn)\n",
    "    del df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mszinte",
   "language": "python",
   "name": "mszinte"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
